{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set we are going to be developing our own custom data scrappers, use some other libraries to scrape data, and finally we will be utilizing **Amazon’s Web Services** to save data on the cloud. We will also use a `virtual machine` on **AWS** and continuously scrape data, without having to have our own computer on the whole time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s start by setting up an **AWS** account. **AWS** provides a number of cloud services that make it very easy to setup remote processes, and take advantage of cloud computing. Their built-in console facilitates the creation of instances of services such as remote servers, and web-based storage. We will be using **EC2** and **S3**. **EC2** allows us to create virtual servers, so we can run scheduled tasks without having to run them continuously on our computer. **S3** provides storage space on the cloud. We can easily save any scrapped data into a **bucket** or data container for future use.\n",
    "\n",
    "AWS offers a [free tier]( https://aws.amazon.com/free) of services. Most of what we will be using in class should be covered by the free tier of usage. Once we create an account, we can set up an **EC2** server, or an **S3** bucket. \n",
    "\n",
    "Once you have setup your AWS account, let’s create a S3 Bucket. We first need to sign into our account and log into our **AWS Console**. Once we can see the console, we need to click on S3 (under storage and content delivery). \n",
    "\n",
    "![AWS Console](images/aws.PNG)\n",
    "\n",
    "S3 has its own sub-console, where we can create and manage buckets. We can think of a bucket as a directory within our hard drive. We will need to be specific about the name we use for our bucket, since we will need this name to store files remotely in it. \n",
    "\n",
    "![S3 Console](images/s3.PNG)\n",
    "\n",
    "Creating a bucket is very easy! We click on `create bucket`, and we choose a name and region. For our purposes, the region is not relevant, so we can just use the `US Standard`.  \n",
    "\n",
    "![Create Bucket](images/create bucket.PNG)\n",
    "\n",
    "Once we are done setting up our **AWS** account and buckets, we need to setup our security credentials. On the console, click on your name, and then on **security credentials**.\n",
    "\n",
    "![Create Bucket](images/aws setup.PNG)\n",
    "\n",
    "Next, click on **Access Keys**. And on **Create Key**. You can only have up to 2 active security keys.\n",
    "\n",
    "![Create Bucket](images/aws security.PNG)\n",
    "\n",
    "Once you create your **Access Key**, you will get a **Key File**. Make sure to download it, store it in a safe place, and to not share it with anyone. We will use these keys to access our buckets with `Boto`. \n",
    "\n",
    "![Create Bucket](images/create key.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a scrapper for foursquare, and a scrapper for instagram. Lets install the libraries from the command line:\n",
    "\n",
    "```Python\n",
    "# Python library for accessing foursquare's API\n",
    "pip install foursquare\n",
    "\n",
    "# Python library for accessing instagram's API\n",
    "pip install python-instagram\n",
    "\n",
    "# Python library for interfacing with AWS\n",
    "pip install boto \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Following the instructions found in the in-class tutorial, and python foursquare's [**API**](https://github.com/mLewisLogic/foursquare), create a scraper that continuously gets the trending venues in Riyadh. \n",
    "\n",
    "\n",
    "* First, make sure to [create your developer passwords]( https://foursquare.com/developers/apps) for foursquare. You will need them to authenticate. We will need to create a foursquare account, and then create an app. The foursquare API has specific limits of usage and requests. Every app we create had its own limits, so to avoid reaching the request limits, we will benefit from using a different app name for every scrapper. Once we log into the foursquare developer console, click on **create a new app**.\n",
    "\n",
    "![S3 Console](images/foursquare.PNG)\n",
    "\n",
    "For the scope of this scrapper and project, we don’t need to worry about having a page url, or a redirect URI. So we can just use a random webpage. All we need to worry about is the app name. This will enable us to obtain access credentials to the API.\n",
    "\n",
    "![S3 Console](images/foursquare2.PNG)\n",
    "\n",
    "* Second, look at [trending]( https://developer.foursquare.com/docs/venues/trending) end point of Foursquare’s **API**. Endpoints allow access to some of the resources from the API. More documentation can be accessed [here]( https://developer.foursquare.com/docs/). The python library provides an easy to use wrapped around the foursquare **API**, so we don’t need to come up with **REST** requests ourselves. \n",
    "\n",
    "* Third, create a python function that uses the python library, and authenticates you with the key, and hits the API returning a number of trending venues.\n",
    "    1. Inputs: \n",
    "      1. A `string` that represents a location in Riyadh. You might want to use a point close to the city center. \n",
    "      * Limit the search to 1,000 results, and a 5,000 mt. radius.\n",
    "    2. Outputs:\n",
    "      1. A `response dictionary` returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fourth, using [boto]( https://github.com/boto/boto), a python wrapper for **AWS**, create a python function that can upload a string to **AWS** [**S3**]( https://aws.amazon.com/s3/). S3 provides cloud buckets where you can store information such as `JSON`s or `strings`. First, you will need to [sign up]( https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?nc2=h_ct) for an account (you get some free services, and 5 gb of free storage).  Then, you will have to get [access keys]( http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html).\n",
    "\n",
    "**The function should:**\n",
    "* Create an [S3 connection]( http://docs.pythonboto.org/en/latest/ref/s3.html)\n",
    "* Select a bucket from your S3 account (You should previously [create]( http://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) the S3 bucket). \n",
    "* Upload a string to a target path that is **specific** to that string. IE you can use the **date and time** as the path, to make sure no 2 files are the same. \n",
    "\n",
    "    1. Inputs: \n",
    "      1. A `string` that represents the target path to upload the string.\n",
    "      * A `string` which is a `json` containing the response from the endpoint.\n",
    "    2. Outputs:\n",
    "      1. A `printed` message of success if uploaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fifth, a function that consecutively hits foursquare’s **API** using the first function you built. The function should hit the API every `N` time, for a given period of time. You can use a library like [threading]( https://docs.python.org/2/library/threading.html) to repeat the operation for given period of time. The function should also parse the response into a dictionary, and keep the following keys:\n",
    "\n",
    "```Python\n",
    "                checkin['name']\n",
    "                checkin['hereNow']['count']\n",
    "                checkin['categories'][0]['name']\n",
    "                checkin['location']['lat']\n",
    "                checkin['location']['lng']\n",
    "                checkin['stats']['checkinsCount']\n",
    "                checkin['stats']['usersCount']\n",
    "                datetime\n",
    "                \n",
    "```\n",
    "\n",
    "1. Inputs: \n",
    "    1. A `number` that defines the total time to run the function for.\n",
    "2. Outputs:\n",
    "    1. A `json` of the parsed response.\n",
    "    2. Use your S3 function to upload it to your bucket. Add a screenshot of your bucket with the files to the ipython notebook.\n",
    "    \n",
    "`The code will be run on the server we just created, so the functions should be well defined and easy to run.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Following the instructions found in the in-class tutorial, and python instagram’s [**API**](https://github.com/Instagram/python-instagram), create a scraper that continuously gets the recent media in Riyadh. \n",
    "\n",
    "\n",
    "* First, make sure to [create your developer passwords](https://www.instagram.com/developer/) for Instagram. You will need them to authenticate.\n",
    "\n",
    "* Second, look at the [location](https://www.instagram.com/developer/endpoints/locations/) end points of Instagram’s **API**. \n",
    "\n",
    "* Third, authenticate into the instagram API. Then, create a python function that uses the python instagram library, and hits the API returning a number of recent media by location.\n",
    "    1. Inputs: \n",
    "      1. A `list` that contains a number of Instagram place id’s in Riyadh. \n",
    "    2. Outputs:\n",
    "      1. A `response` [**media**](https://github.com/Instagram/python-instagram/blob/master/instagram/models.py#L46) object.\n",
    "      \n",
    "Python’s **Instagram** library provides an easy access to instagram’s API. The library returns a number of objects that already parse the API’s response very nicely. We can then access the properties of the object with a simple syntax. For example, to access the user name of a given media object we just have to do:\n",
    "\n",
    "```Python\n",
    "user_name = my_instagram.user\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fourth, create a python function that uses the python instagram library, and hits the API to search for location id’s. You can use the `location_search` function of the library. This should return a list of places that you can then use to search for recent media.\n",
    "    1. Inputs: \n",
    "      1. Two location `float` numbers representing the lat and lon of Riyadh.\n",
    "      * Limit the search to 5,000 results, and a 5,000 mt. radius.\n",
    "    2. Outputs:\n",
    "      1. A `list` of Instagram locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fifth, a function that consecutively hitsinstagram’s **API** using the first function you built. The function should hit the API every `N` time, for a given period of time. You can use a library like [threading]( https://docs.python.org/2/library/threading.html) to repeat the operation for given period of time. The function should also parse the response object into a dictionary, and keep the keys that are relevant to you:\n",
    "\n",
    "1. Inputs: \n",
    "    1. A `number` that defines the total time to run the function for.\n",
    "    2. A `list` of locations that will be used to run your previous function.\n",
    "\n",
    "2. Outputs:\n",
    "    1. A `json` of the parsed response.\n",
    "    2. Use your S3 function to upload it to your bucket. Add a screenshot of your bucket with the files to the ipython notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Now, lets read some of the jsons we have been collecting, and create a quick plot based on their geo-location and some of their basic information.  \n",
    "\n",
    "\n",
    "* Post a screenshot of your S3 bucket on your ipython notebook. You can post screenshots by using the following `markdown` syntax:\n",
    "\n",
    "```markdown\n",
    "![Caption](path to the image)\n",
    "```\n",
    "\n",
    "* Download the foursquare files from your S3 bucket. You can use a UI like [S3 Browser](http://s3browser.com/), or write a Boto function.\n",
    "\n",
    "* Create a function that looks for all the files within a given local directory, and opens them. Here is some documentation: [list directory](https://docs.python.org/2/library/os.html#os.listdir), [open files](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files) \n",
    "\n",
    "* Third, plot the latitude and longitude of each **unique** venue. The radius of the circle should be proportional to the number of check-ins per venue. \n",
    "    1. Inputs: \n",
    "      1. A `string` that represents a file path containing all the jsons from S3. Alternatively, you can just write a function to download the files from S3 on real time by passing your credentials, and a bucket name.  \n",
    "    2. Outputs:\n",
    "      1. A plot with the unique venues represented by a scatter plot. The radius of each circle should be related to the number of check-ins per venue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
